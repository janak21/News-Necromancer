"""
Audio cache manager for voice narration files.

Implements LRU eviction and TTL-based cleanup for efficient audio file caching.
"""

import hashlib
import json
import aiofiles
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from dataclasses import dataclass, asdict
from backend.models.narration_models import VoiceStyleEnum


@dataclass
class CacheEntry:
    """Metadata for a cached audio file"""
    narration_id: str
    file_path: str
    created_at: str  # ISO format datetime string
    last_accessed: str  # ISO format datetime string
    file_size: int  # bytes
    variant_id: str
    voice_style: str
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CacheEntry':
        """Create CacheEntry from dictionary"""
        return cls(**data)


class AudioCacheManager:
    """
    Manages audio file caching with LRU eviction and TTL-based cleanup.
    
    Features:
    - SHA-256 based cache keys from variant_id + voice_style
    - LRU eviction when cache exceeds max_size_mb
    - TTL-based cleanup for entries older than ttl_days
    - Metadata tracking for cache management
    """
    
    def __init__(
        self,
        cache_dir: Path,
        max_size_mb: int = 500,
        ttl_days: int = 7
    ):
        """
        Initialize the audio cache manager.
        
        Args:
            cache_dir: Directory to store cached audio files
            max_size_mb: Maximum cache size in megabytes (default: 500MB)
            ttl_days: Time-to-live for cache entries in days (default: 7 days)
        """
        self.cache_dir = Path(cache_dir)
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.ttl = timedelta(days=ttl_days)
        self.cache_index: Dict[str, CacheEntry] = {}
        self.index_file = self.cache_dir / "cache_index.json"
        
        # Create cache directory if it doesn't exist
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Load existing cache index
        self._load_index()
    
    def get_cache_key(
        self,
        variant_id: str,
        voice_style: VoiceStyleEnum
    ) -> str:
        """
        Generate unique cache key using SHA-256 hash.
        
        Args:
            variant_id: ID of the spooky variant
            voice_style: Voice style enum value
            
        Returns:
            SHA-256 hash string as cache key
        """
        content = f"{variant_id}:{voice_style.value}"
        return hashlib.sha256(content.encode()).hexdigest()
    
    async def get(self, cache_key: str) -> Optional[Path]:
        """
        Retrieve cached audio file if it exists and is valid.
        
        Args:
            cache_key: Cache key generated by get_cache_key()
            
        Returns:
            Path to cached audio file, or None if not found/expired
        """
        # Check if entry exists in index
        if cache_key not in self.cache_index:
            return None
        
        entry = self.cache_index[cache_key]
        file_path = Path(entry.file_path)
        
        # Check if file actually exists
        if not file_path.exists():
            # Remove stale entry
            del self.cache_index[cache_key]
            await self._save_index()
            return None
        
        # Check if entry has expired (TTL)
        created_at = datetime.fromisoformat(entry.created_at)
        age = datetime.now() - created_at
        
        if age > self.ttl:
            # Entry expired, remove it
            await self._remove_entry(cache_key)
            return None
        
        # Update last accessed time
        entry.last_accessed = datetime.now().isoformat()
        await self._save_index()
        
        return file_path
    
    async def put(
        self,
        cache_key: str,
        audio_data: bytes,
        metadata: Dict[str, Any]
    ) -> Path:
        """
        Store audio file in cache with metadata.
        
        Args:
            cache_key: Cache key for the audio file
            audio_data: Audio file content as bytes
            metadata: Dictionary containing variant_id, voice_style, narration_id
            
        Returns:
            Path to the stored audio file
        """
        # Calculate required space
        required_space = len(audio_data)
        
        # Check if we need to evict entries
        current_size = self._get_total_cache_size()
        if current_size + required_space > self.max_size_bytes:
            await self.evict_lru(required_space)
        
        # Generate file path
        file_path = self.cache_dir / f"{cache_key}.mp3"
        
        # Write audio file
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(audio_data)
        
        # Create cache entry
        now = datetime.now().isoformat()
        entry = CacheEntry(
            narration_id=metadata.get('narration_id', cache_key),
            file_path=str(file_path),
            created_at=now,
            last_accessed=now,
            file_size=len(audio_data),
            variant_id=metadata['variant_id'],
            voice_style=metadata['voice_style']
        )
        
        # Add to index
        self.cache_index[cache_key] = entry
        await self._save_index()
        
        return file_path
    
    async def evict_lru(self, required_space: int):
        """
        Remove least recently used entries to free up space.
        
        Args:
            required_space: Number of bytes needed
        """
        # Sort entries by last_accessed (oldest first)
        sorted_entries = sorted(
            self.cache_index.items(),
            key=lambda x: datetime.fromisoformat(x[1].last_accessed)
        )
        
        freed_space = 0
        entries_to_remove = []
        
        # Remove entries until we have enough space
        for cache_key, entry in sorted_entries:
            if freed_space >= required_space:
                break
            
            entries_to_remove.append(cache_key)
            freed_space += entry.file_size
        
        # Remove the selected entries
        for cache_key in entries_to_remove:
            await self._remove_entry(cache_key)
    
    async def cleanup_expired(self):
        """
        Remove all cache entries older than TTL.
        
        This method should be called periodically to clean up expired entries.
        """
        now = datetime.now()
        expired_keys = []
        
        for cache_key, entry in self.cache_index.items():
            created_at = datetime.fromisoformat(entry.created_at)
            age = now - created_at
            
            if age > self.ttl:
                expired_keys.append(cache_key)
        
        # Remove expired entries
        for cache_key in expired_keys:
            await self._remove_entry(cache_key)
    
    def _get_total_cache_size(self) -> int:
        """
        Calculate total size of all cached files.
        
        Returns:
            Total size in bytes
        """
        return sum(entry.file_size for entry in self.cache_index.values())
    
    async def _remove_entry(self, cache_key: str):
        """
        Remove a cache entry and its associated file.
        
        Args:
            cache_key: Cache key to remove
        """
        if cache_key not in self.cache_index:
            return
        
        entry = self.cache_index[cache_key]
        file_path = Path(entry.file_path)
        
        # Delete the file if it exists
        if file_path.exists():
            file_path.unlink()
        
        # Remove from index
        del self.cache_index[cache_key]
        await self._save_index()
    
    def _load_index(self):
        """Load cache index from disk."""
        if not self.index_file.exists():
            self.cache_index = {}
            return
        
        try:
            with open(self.index_file, 'r') as f:
                data = json.load(f)
                self.cache_index = {
                    key: CacheEntry.from_dict(entry_data)
                    for key, entry_data in data.items()
                }
        except (json.JSONDecodeError, KeyError, TypeError):
            # If index is corrupted, start fresh
            self.cache_index = {}
    
    async def _save_index(self):
        """Save cache index to disk."""
        data = {
            key: entry.to_dict()
            for key, entry in self.cache_index.items()
        }
        
        async with aiofiles.open(self.index_file, 'w') as f:
            await f.write(json.dumps(data, indent=2))
